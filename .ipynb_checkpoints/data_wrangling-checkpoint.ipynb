{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cabd662e9e43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as fn\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2260669 entries, 0 to 2260668\n",
      "Columns: 145 entries, loan.csv to settlement_term\n",
      "dtypes: float64(109), object(36)\n",
      "memory usage: 2.4+ GB\n"
     ]
    }
   ],
   "source": [
    "('spark.executor.memory', '8g'),('spark.driver.memory','8g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kip/anaconda3/lib/python3.7/site-packages/pandas/core/ops.py:1167: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = method(y)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "invalid type comparison",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b45891581c49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloans_data_null_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloan_data_pd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloan_data_pd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettlement_status\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'NaN'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mloans_data_null_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, other, axis)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1283\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mna_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1284\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m                 raise TypeError('Could not compare {typ} type with Series'\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mna_op\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"invalid type comparison\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1170\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: invalid type comparison"
     ]
    }
   ],
   "source": [
    "sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_files = glob.glob('*.gz')\n",
    "raw_data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File location and type\n",
    "file_location_loans = \"loan.csv.tar.gz\"\n",
    "\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"true\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_schema = StructType([StructField('id',StringType()),\n",
    "StructField('member_id', StringType()),\n",
    "StructField('loan_amnt', IntegerType()),\n",
    "StructField('funded_amnt', IntegerType()),\n",
    "StructField('funded_amnt_inv', DoubleType()),\n",
    "StructField('term', StringType()),\n",
    "StructField('int_rate', DoubleType()),\n",
    "StructField('installment', DoubleType()),\n",
    "StructField('grade', StringType()),\n",
    "StructField('sub_grade', StringType()),\n",
    "StructField('emp_title', StringType()),\n",
    "StructField('emp_length', StringType()),\n",
    "StructField('home_ownership', StringType()),\n",
    "StructField('annual_inc', StringType()),\n",
    "StructField('verification_status', StringType()),\n",
    "StructField('issue_d', StringType()),\n",
    "StructField('loan_status', StringType()),\n",
    "StructField('pymnt_plan', StringType()),\n",
    "StructField('url', StringType()),\n",
    "StructField('desc', StringType()),\n",
    "StructField('purpose', StringType()),\n",
    "StructField('title', StringType()),\n",
    "StructField('zip_code', StringType()),\n",
    "StructField('addr_state', StringType()),\n",
    "StructField('dti', StringType()),\n",
    "StructField('delinq_2yrs', StringType()),\n",
    "StructField('earliest_cr_line', StringType()),\n",
    "StructField('inq_last_6mths', StringType()),\n",
    "StructField('mths_since_last_delinq', StringType()),\n",
    "StructField('mths_since_last_record', StringType()),\n",
    "StructField('open_acc', StringType()),\n",
    "StructField('pub_rec', StringType()),\n",
    "StructField('revol_bal', StringType()),\n",
    "StructField('revol_util', StringType()),\n",
    "StructField('total_acc', StringType()),\n",
    "StructField('initial_list_status', StringType()),\n",
    "StructField('out_prncp', StringType()),\n",
    "StructField('out_prncp_inv', StringType()),\n",
    "StructField('total_pymnt', StringType()),\n",
    "StructField('total_pymnt_inv', StringType()),\n",
    "StructField('total_rec_prncp', StringType()),\n",
    "StructField('total_rec_int', StringType()),\n",
    "StructField('total_rec_late_fee', StringType()),\n",
    "StructField('recoveries', StringType()),\n",
    "StructField('collection_recovery_fee', StringType()),\n",
    "StructField('last_pymnt_d', StringType()),\n",
    "StructField('last_pymnt_amnt', StringType()),\n",
    "StructField('next_pymnt_d', StringType()),\n",
    "StructField('last_credit_pull_d', StringType()),\n",
    "StructField('collections_12_mths_ex_med', StringType()),\n",
    "StructField('mths_since_last_major_derog', StringType()),\n",
    "StructField('policy_code', StringType()),\n",
    "StructField('application_type', StringType()),\n",
    "StructField('annual_inc_joint', StringType()),\n",
    "StructField('dti_joint', StringType()),\n",
    "StructField('verification_status_joint', StringType()),\n",
    "StructField('acc_now_delinq', StringType()),\n",
    "StructField('tot_coll_amt', StringType()),\n",
    "StructField('tot_cur_bal', StringType()),\n",
    "StructField('open_acc_6m', StringType()),\n",
    "StructField('open_act_il', StringType()),\n",
    "StructField('open_il_12m', StringType()),\n",
    "StructField('open_il_24m', StringType()),\n",
    "StructField('mths_since_rcnt_il', StringType()),\n",
    "StructField('total_bal_il', StringType()),\n",
    "StructField('il_util', StringType()),\n",
    "StructField('open_rv_12m', StringType()),\n",
    "StructField('open_rv_24m', StringType()),\n",
    "StructField('max_bal_bc', StringType()),\n",
    "StructField('all_util', StringType()),\n",
    "StructField('total_rev_hi_lim', StringType()),\n",
    "StructField('inq_fi', StringType()),\n",
    "StructField('total_cu_tl', StringType()),\n",
    "StructField('inq_last_12m', StringType()),\n",
    "StructField('acc_open_past_24mths', IntegerType()),\n",
    "StructField('avg_cur_bal', StringType()),\n",
    "StructField('bc_open_to_buy', IntegerType()),\n",
    "StructField('bc_util', StringType()),\n",
    "StructField('chargeoff_within_12_mths', DoubleType()),\n",
    "StructField('delinq_amnt', IntegerType()),\n",
    "StructField('mo_sin_old_il_acct', DoubleType()),\n",
    "StructField('mo_sin_old_rev_tl_op', IntegerType()),\n",
    "StructField('mo_sin_rcnt_rev_tl_op', IntegerType()),\n",
    "StructField('mo_sin_rcnt_tl', IntegerType()),\n",
    "StructField('mort_acc', IntegerType()),\n",
    "StructField('mths_since_recent_bc', IntegerType()),\n",
    "StructField('mths_since_recent_bc_dlq', IntegerType()),\n",
    "StructField('mths_since_recent_inq', IntegerType()),\n",
    "StructField('mths_since_recent_revol_delinq', IntegerType()),\n",
    "StructField('num_accts_ever_120_pd', IntegerType()),\n",
    "StructField('num_actv_bc_tl', IntegerType()),\n",
    "StructField('num_actv_rev_tl', IntegerType()),\n",
    "StructField('num_bc_sats', IntegerType()),\n",
    "StructField('num_bc_tl', IntegerType()),\n",
    "StructField('num_il_tl', IntegerType()),\n",
    "StructField('num_op_rev_tl', IntegerType()),\n",
    "StructField('num_rev_accts', IntegerType()),\n",
    "StructField('num_rev_tl_bal_gt_0', IntegerType()),\n",
    "StructField('num_sats', IntegerType()),\n",
    "StructField('num_tl_120dpd_2m', IntegerType()),\n",
    "StructField('num_tl_30dpd', IntegerType()),\n",
    "StructField('num_tl_90g_dpd_24m', IntegerType()),\n",
    "StructField('num_tl_op_past_12m', IntegerType()),\n",
    "StructField('pct_tl_nvr_dlq', DoubleType()),\n",
    "StructField('percent_bc_gt_75', DoubleType()),\n",
    "StructField('pub_rec_bankruptcies', IntegerType()),\n",
    "StructField('tax_liens', IntegerType()),\n",
    "StructField('tot_hi_cred_lim', IntegerType()),\n",
    "StructField('total_bal_ex_mort', IntegerType()),\n",
    "StructField('total_bc_limit', IntegerType()),\n",
    "StructField('total_il_high_credit_limit', IntegerType()),\n",
    "StructField('revol_bal_joint', IntegerType()),\n",
    "StructField('sec_app_earliest_cr_line', StringType()),\n",
    "StructField('sec_app_inq_last_6mths', IntegerType()),\n",
    "StructField('sec_app_mort_acc', IntegerType()),\n",
    "StructField('sec_app_open_acc', IntegerType()),\n",
    "StructField('sec_app_revol_util', DoubleType()),\n",
    "StructField('sec_app_open_act_il', IntegerType()),\n",
    "StructField('sec_app_num_rev_accts', IntegerType()),\n",
    "StructField('sec_app_chargeoff_within_12_mths', IntegerType()),\n",
    "StructField('sec_app_collections_12_mths_ex_med', IntegerType()),\n",
    "StructField('sec_app_mths_since_last_major_derog', IntegerType()),\n",
    "StructField('hardship_flag', StringType()),\n",
    "StructField('hardship_type', StringType()),\n",
    "StructField('hardship_reason', StringType()),\n",
    "StructField('hardship_status', StringType()),\n",
    "StructField('deferral_term', StringType()),\n",
    "StructField('hardship_amount', StringType()),\n",
    "StructField('hardship_start_date', StringType()),\n",
    "StructField('hardship_end_date', StringType()),\n",
    "StructField('payment_plan_start_date', StringType()),\n",
    "StructField('hardship_length', StringType()),\n",
    "StructField('hardship_dpd', StringType()),\n",
    "StructField('hardship_loan_status', StringType()),\n",
    "StructField('orig_projected_additional_accrued_interest', StringType()),\n",
    "StructField('hardship_payoff_balance_amount', StringType()),\n",
    "StructField('hardship_last_payment_amount', StringType()),\n",
    "StructField('disbursement_method', StringType()),\n",
    "StructField('debt_settlement_flag', StringType()),\n",
    "StructField('debt_settlement_flag_date', StringType()),\n",
    "StructField('settlement_status', StringType()),\n",
    "StructField('settlement_date', StringType()),\n",
    "StructField('settlement_amount', StringType()),\n",
    "StructField('settlement_percentage', StringType()),\n",
    "StructField('settlement_term', StringType())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_df2 = (spark.read.format(file_type)   \n",
    "                    .option(\"mode\", \"DROPMALFORMED\")\n",
    "                    .option(\"sep\", delimiter)\n",
    "                    .schema(loan_schema)\n",
    "                    .load(file_location_loans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first column\n",
    "loans_df2.select(loans_df2.columns[0]).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Id column contains null values except the first row which contains some file details. Let's drop the first row containing the file text details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_df3 = loans_df2.drop(loans_df2['id'] != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_df3.select(loans_df3.columns[0:10]).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm we dropped only the one row by taking a look at the count of rows in our new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_observations = loans_df3.count()\n",
    "total_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_observations , loans_df3.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a monotonically increasing id then drop the null id column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_uniqueId = (\n",
    "        loans_df2\n",
    "        \n",
    "        .select(\n",
    "            [fn.monotonically_increasing_id().alias('id')] + [col for col in loans_df2.columns if col != 'id'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_uniqueId.count(), loans_uniqueId.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we still have duplicates. Lets find which rows are duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = (loans_uniqueId\n",
    "                 .groupby([col for col in loans_uniqueId.columns if col !='id'])\n",
    "                 .count()\n",
    "                 .filter('count > 1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_uniqueId.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too many columns makes it heard to cleary see the values in the table. We can select only the first 10 columns to view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_loans = loans_uniqueId.select(loans_uniqueId.columns[0:10]).show(5)\n",
    "sample_loans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "member_id is null, we need to remove null columns from our dataset. Let us calculate the percentage of NaN and Null values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, isnan, lit, sum\n",
    "def count_not_null(c, nan_as_null=False):\n",
    "    \"\"\"Use conversion between boolean and integer\n",
    "    - False -> 0\n",
    "    - True ->  1\n",
    "    \"\"\"\n",
    "    pred = col(c).isNotNull() & (~isnan(c) if nan_as_null else lit(True))\n",
    "    return sum(pred.cast(\"integer\")).alias(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exprs = [(count_not_null(c, True) / count(\"*\")).alias(c) for c in loans_uniqueId.columns]\n",
    "loans_null_percentage = loans_uniqueId.agg(*exprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_null_percentage.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is a number of columns with percentage of NaNs and Nulls greater than 30%. Let us see these columns in a clear format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = loans_null_percentage.toPandas()\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = {}\n",
    "cols_to_drop = []\n",
    "for c in loans_null_percentage.columns:\n",
    "    if results[c].iloc[0] < 0.3:\n",
    "        my_dict[c] = results[c].iloc[0]\n",
    "        cols_to_drop.append(c)\n",
    "len(my_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list of all columns with NANs greater than 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_pd = pd.Series(my_dict)\n",
    "results_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the 40 columns which contains at least 30% null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_nulls_removed = loans_uniqueId.drop(*cols_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print the schema of the data we are left with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_nulls_removed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "total columns left "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(loans_nulls_removed.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
